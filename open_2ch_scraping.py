from bs4 import BeautifulSoup as bs
import cloudscraper
import pandas as pd
import re
import time

class scrape_onj:

    def __init__(self):
            self.scraper = cloudscraper.create_scraper(delay=1, browser="chrome") #cloudflareã‚µãƒ¼ãƒãƒ¼ã®1020ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’é˜²ããŸã‚requestsã§ã¯ãªãcloudscraperã‚’ç”¨ã„ã‚‹ã€‚å°šã€ãŠãƒ¼ã·ã‚“2chã§ã¯ãƒ—ãƒ­ã‚¯ã‚·ã¯ä½¿ç”¨ã§ããªã„ã€‚
        
    def get_links():
    #ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä½œæ¥­ã‚’ã¾ãšã¯ãŠã‚“Jã®ã‚¹ãƒ¬ãƒƒãƒ‰ä¸€è¦§ã§è¡Œã„ã€ãƒªãƒ³ã‚¯ã‚’æ‰‹ã«å…¥ã‚Œã‚‹ã€‚
        links = list()
        html_ll = scrape_onj.scraper.get('https://hayabusa.open2ch.net/headline.cgi?bbs=livejupiter') 
        soup_ll = bs(html_ll.content,'html.parser')
        tags = soup_ll('a')
        for tag in tags:
             links.append(tag.get('href',None))
        return links

    def parse_thread():
        df_list = list()
        temp_dict = dict()
        links = scrape_onj.get_links()
        for link in links:
                if len(link) < 2:
                        continue
                r = scrape_onj.scraper.get(link)
                soup = bs(r.content,'html.parser')
                main_wrap = soup(class_ ='MAIN_WRAP')
                for tag in main_wrap:
                        temp_dict['title'] = soup.h1.text
                        temp_dict['comments'] = soup.dl.dd.text
                        icchidatas = soup.dl.dt.text
                        nanashi = re.findall('1 ï¼š(...)',icchidatas)
                        date = re.findall(r'\d*/\d*/\d*',icchidatas)
                        timetable= re.findall(r'\d*:\d*:\d*',icchidatas)
                        id = re.findall('ID:(....)',icchidatas)
                        temp_dict['nanashi'] = nanashi[0]
                        temp_dict['date'] = date[0]
                        temp_dict['timetable'] = timetable[0]
                        temp_dict['id'] = id[0]
                df_list.append(temp_dict)
                temp_dict = {}
        time.sleep(1)
                
        thereads_df = pd.DataFrame(df_list)
        thereads_df.to_pickle('C:\Users\Clean\Documents\workspace\easy_scraping\data\threads.pkl')
        
    def parse_res(self):  
        #æŒ‡å®šã—ãŸã‚¹ãƒ¬ãƒƒãƒ‰å†…ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ã€ãƒ¬ã‚¹ã ã‘ã‚’æŠ½å‡ºã—ã¦ã„ãã€‚ã“ã£ã¡ã¯ä¸€ã¤ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã«å¯¾ã—ã¦è¡Œã†
        while True:
                add = input('ãƒ¬ã‚¹ã‚’é›†ã‚ãŸã„ã‚¹ãƒ¬ã®URLã‚’å…¥åŠ›ã—ã¦ãã‚Œã‚„*çµ‚äº†ã—ãŸã„æ™‚ã¯qå…¥åŠ›ã—ã¦ãª:')
                if len(add) < 1:
                        print('URLå…¥åŠ›ã—ã¦ãã‚Œã‚„ï¼›ï¼›')
                        continue
                if add == 'q' or add == 'Q':
                        print('ã”åˆ©ç”¨ã‚ã‚ŠãŒã¨ã”ã–ã¾ã™ğŸ¥º')
                        quit()
                res = self.scraper.get()
        
        
#scrape_onj.parse_thread()